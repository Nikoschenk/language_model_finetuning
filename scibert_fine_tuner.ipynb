{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scibert-fine-tuner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikoschenk/language_model_finetuning/blob/master/scibert_fine_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK_lPvRlBdMB",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning Transformers on [CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
        "This notebook demonstrates how to fine-tune [Sci-Bert](https://github.com/allenai/scibert) on the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) using the [Hugging Face Transformer](https://github.com/huggingface/transformers) library.\n",
        "\n",
        "Credits: Modification of [original notebook](https://colab.research.google.com/github/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/hw4_transformer.ipynb) provided by [Daphne Ippolito](https://www.seas.upenn.edu/~daphnei/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV1z0koaDHT",
        "colab_type": "text"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGscdaCtpmbV",
        "colab_type": "text"
      },
      "source": [
        "### Install SciBert and HuggingFace Transfomers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Da-Wadvijh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download SciBERT model and extract it in your local drive.\n",
        "!wget -nc -O /content/scibert_scivocab_uncased.tar https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n",
        "!ls /content\n",
        "\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "!echo \"Extracting SciBERT ...\"\n",
        "!tar -xvf scibert_scivocab_uncased.tar\n",
        "!echo \"... done.\"\n",
        "\n",
        "# Delete tar archive.\n",
        "#!rm /content/scibert_scivocab_uncased.tar\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install transformers.\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "# Use language modeling version as of April 21st.\n",
        "!git checkout b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead\n",
        "\n",
        "# Make sure that this version of transformers has the correct evaluate() function.\n",
        "from run_language_modeling import evaluate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2LEl-uwMXp",
        "colab_type": "text"
      },
      "source": [
        "### Mount your Google Drive\n",
        "Checkpoints will be saved Google Drive folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ6FFHiMMP0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuEqWOhwn-a",
        "colab_type": "text"
      },
      "source": [
        "### Download the CORD-19 data.\n",
        "You need to prepare the data yourself (as we cannot share it for legal reasons), and split it into training, development and test sets.\n",
        "We've left dummy data for experimentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GzNXh7ap_R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the data.\n",
        "# Credits: http://interactive-fiction-class.org/staff.html\n",
        "!wget -nc -O /content/presidential_speeches_test.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_test.txt\n",
        "!wget -nc -O /content/presidential_speeches_valid.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_valid.txt\n",
        "!wget -nc -O /content/presidential_speeches_train.txt https://raw.githubusercontent.com/interactive-fiction-class/interactive-fiction-class.github.io/master/homeworks/language-model/presidential_speeches_train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzV8iTrphJl",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning and Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzFhwDSqOg3",
        "colab_type": "text"
      },
      "source": [
        "### Launch fine-tuninng\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IByoOWzAPrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine-tune SciBERT on your custom text data.\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/SciBERT-finetuned' \\\n",
        "    --model_type=bert \\\n",
        "    --model_name_or_path='/content/scibert_scivocab_uncased/' \\\n",
        "    --mlm \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=3.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=500 \\\n",
        "    --train_data_file=/content/presidential_speeches_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/presidential_speeches_valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size=512 \\\n",
        "    --gradient_accumulation_steps=5\n",
        "# Not sure if adding explicit tokenization has an effect.\n",
        "#--tokenizer_name=bert-base-uncased \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm",
        "colab_type": "text"
      },
      "source": [
        "### Compute perplexity of a dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSgGhtcDNd",
        "colab_type": "text"
      },
      "source": [
        "#### Check which (fine-tuned) checkpoints are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_qHytBIETo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/SciBERT-finetuned/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None\n",
        "  )\n",
        "  \n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "  args.eval_data_file=data_file_path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb",
        "colab_type": "text"
      },
      "source": [
        "#### Compute perplexity scores between original and fine-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJzbBpE_IZlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# SciBERT evaluation.\n",
        "# 1.) Path to one of your fine-tuned models.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/SciBERT-finetuned/checkpoint-500'\n",
        "# Uncomment for comparison with original SciBERT model.\n",
        "#CHECKPOINT_PATH = '/content/scibert_scivocab_uncased/'\n",
        "# Uncomment for comparison with base model.\n",
        "#CHECKPOINT_PATH = 'bert-base-uncased'\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/presidential_speeches_valid.txt\",\n",
        "              \"/content/presidential_speeches_test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "#print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 512,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=True,\n",
        "  mlm_probability=0.15,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='bert',\n",
        "  seed=42,\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print(\"\\n\")\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}